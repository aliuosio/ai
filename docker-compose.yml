
services:
  

  ollama:G
    image: alpine/ollama:latest
    container_name: ${NAMESPACE}_ollama
    ports:
      - "11434:11434"
    env_file: .env
    tty: true
    init: true
    pull_policy: always
    volumes:
      - ~/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities:
                - gpu

  open-webui:
    container_name: ${NAMESPACE}_open-webui
    image: ghcr.io/open-webui/open-webui:latest-cuda
    pull_policy: always
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    env_file:
      - .env  
    volumes:
      - ~/.webui:/app/backend/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities:
                - gpu